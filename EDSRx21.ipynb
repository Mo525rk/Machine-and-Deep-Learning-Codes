{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvNKAzHHmD8g"
   },
   "source": [
    "# Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Fsq-GMrl7mB"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import argparse\n",
    "from tensorflow.python.client import device_lib\n",
    "from __future__ import print_function\n",
    "\n",
    "class data_utils:\n",
    "    def getpaths(path):\n",
    "        \"\"\"\n",
    "        Get all image paths from folder 'path' while avoiding ._ files.\n",
    "        \"\"\"\n",
    "        im_paths = []\n",
    "        for fil in os.listdir(path):\n",
    "              if '.png' in fil:\n",
    "                if \"._\" in fil:\n",
    "                      #avoid dot underscore\n",
    "                    pass\n",
    "                else:\n",
    "                      im_paths.append(os.path.join(path, fil))\n",
    "      return im_paths\n",
    "\n",
    "    def make_dataset(paths, scale, mean):\n",
    "      \"\"\"\n",
    "      Python generator-style dataset. Creates 48x48 low-res and corresponding high-res patches.\n",
    "      \"\"\"\n",
    "      size_lr = 48\n",
    "      size_hr = size_lr * scale\n",
    "\n",
    "      for p in paths:\n",
    "          # normalize\n",
    "          im_norm = cv2.imread(p.decode(), 3).astype(np.float32) - mean\n",
    "\n",
    "          # random flip\n",
    "          r = random.randint(-1, 2)\n",
    "          if not r == 2:\n",
    "              im_norm = cv2.flip(im_norm, r)\n",
    "\n",
    "          # divisible by scale - create low-res\n",
    "          hr = im_norm[0:(im_norm.shape[0] - (im_norm.shape[0] % scale)),\n",
    "                    0:(im_norm.shape[1] - (im_norm.shape[1] % scale)), :]\n",
    "          lr = cv2.resize(hr, (int(hr.shape[1] / scale), int(hr.shape[0] / scale)),\n",
    "                          interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "          numx = int(lr.shape[0] / size_lr)\n",
    "          numy = int(lr.shape[1] / size_lr)\n",
    "\n",
    "          for i in range(0, numx):\n",
    "              startx = i * size_lr\n",
    "              endx = (i * size_lr) + size_lr\n",
    "\n",
    "              startx_hr = i * size_hr\n",
    "              endx_hr = (i * size_hr) + size_hr\n",
    "\n",
    "              for j in range(0, numy):\n",
    "                  starty = j * size_lr\n",
    "                  endy = (j * size_lr) + size_lr\n",
    "                  starty_hr = j * size_hr\n",
    "                  endy_hr = (j * size_hr) + size_hr\n",
    "\n",
    "                  crop_lr = lr[startx:endx, starty:endy]\n",
    "                  crop_hr = hr[startx_hr:endx_hr, starty_hr:endy_hr]\n",
    "\n",
    "                  x = crop_lr.reshape((size_lr, size_lr, 3))\n",
    "                  y = crop_hr.reshape((size_hr, size_hr, 3))\n",
    "\n",
    "                  yield x, y\n",
    "\n",
    "  def calcmean(imageFolder, bgr):\n",
    "      \"\"\"\n",
    "      Calculates the mean of a dataset.\n",
    "      \"\"\"\n",
    "      paths = getpaths(imageFolder)\n",
    "\n",
    "      total_mean = [0, 0, 0]\n",
    "      im_counter = 0\n",
    "\n",
    "      for p in paths:\n",
    "\n",
    "          image = np.asarray(Image.open(p))\n",
    "\n",
    "          mean_rgb = np.mean(image, axis=(0, 1), dtype=np.float64)\n",
    "\n",
    "          if im_counter % 1000 == 0:\n",
    "              print(\"Total mean: {} | current mean: {}\".format(total_mean, mean_rgb))\n",
    "\n",
    "          total_mean += mean_rgb\n",
    "          im_counter += 1\n",
    "\n",
    "      total_mean /= im_counter\n",
    "\n",
    "      # rgb to bgr\n",
    "      if bgr is True:\n",
    "          total_mean = total_mean[...,::-1]\n",
    "\n",
    "      return total_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JErElIm9lrO-"
   },
   "source": [
    "# Par√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWq3G9M8lvPz"
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #gets rid of avx/fma warning\n",
    "\n",
    "B=32\n",
    "F=256\n",
    "scale=2\n",
    "batch=16\n",
    "epochs=5\n",
    "lr=0.0001\n",
    "load_flag= False\n",
    "mean = [30.7314651342487, 34.951122349740935, 35.78383743336787]\n",
    "ckpt_path = \"/content/drive/MyDrive/CKPT_dirLAST/x2/\"\n",
    "imagefolder=\"/content/drive/MyDrive/TheLastofUs/HD1\"\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaaCFNc_61R9"
   },
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6cX8o_vs4ZA"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "def train(imagefolder):\n",
    "        # Create training dataset\n",
    "        train_image_paths = data_utils.getpaths(imagefolder)\n",
    "        train_dataset = tf.data.Dataset.from_generator(generator=data_utils.make_dataset,\n",
    "                                                 output_types=(tf.float32, tf.float32),\n",
    "                                                 output_shapes=(tf.TensorShape([None, None, 3]), \n",
    "                                                                tf.TensorShape([None, None, 3])),\n",
    "                                                 args=[train_image_paths, scale, mean])\n",
    "        train_dataset = train_dataset.padded_batch(batch, padded_shapes=([None, None, 3],[None, None, 3]))\n",
    "\n",
    "        # Make the iterator and its initializers\n",
    "        output_types=tf.compat.v1.data.get_output_types(train_dataset)\n",
    "        output_shapes=tf.compat.v1.data.get_output_shapes(train_dataset)\n",
    "        train_val_iterator = tf.compat.v1.data.Iterator.from_structure(output_types, output_shapes)\n",
    "        train_initializer = train_val_iterator.make_initializer(train_dataset)\n",
    "\n",
    "        handle = tf.compat.v1.placeholder(tf.string, shape=[])\n",
    "        iterator = tf.compat.v1.data.Iterator.from_string_handle(handle, output_types, output_shapes)\n",
    "        LR, HR = iterator.get_next()\n",
    "\n",
    "        # Edsr model\n",
    "        print(\"\\nRunning EDSR.\")\n",
    "        edsrObj = Edsr(B, F, scale)\n",
    "        out, loss, train_op, psnr, ssim, lr1 = edsrObj.model(x=LR, y=HR, lr=lr)\n",
    "\n",
    "        # -- Training session\n",
    "        with tf.compat.v1.Session(config=config) as sess:\n",
    "\n",
    "            train_writer = tf.compat.v1.summary.FileWriter('/content/drive/MyDrive/logsLAST/train', sess.graph)\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "            saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "            # Create check points directory if not existed, and load previous model if specified.\n",
    "            if not os.path.exists(ckpt_path):\n",
    "                os.makedirs(ckpt_path)\n",
    "            else:\n",
    "                if os.path.isfile(ckpt_path + \"edsr_ckpt\" + \".meta\"):\n",
    "                    if load_flag:\n",
    "                        saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n",
    "                        print(\"\\nLoaded checkpoint.\")\n",
    "                    if not load_flag:\n",
    "                        print(\"No checkpoint loaded. Training from scratch.\")\n",
    "\n",
    "            global_step = 0\n",
    "            tf.convert_to_tensor(global_step)\n",
    "\n",
    "            train_val_handle = sess.run(train_val_iterator.string_handle())\n",
    "\n",
    "            print(\"Training...\")\n",
    "            for e in range(1, epochs+1):\n",
    "\n",
    "                sess.run(train_initializer)\n",
    "                step, train_loss = 0, 0\n",
    "\n",
    "                try:\n",
    "                    while True:\n",
    "                        o, l, t, l_rate = sess.run([out, loss, train_op, lr1], feed_dict={handle:train_val_handle,\n",
    "                                                                                         edsrObj.global_step: global_step})\n",
    "                        train_loss += l\n",
    "                        step += 1\n",
    "                        global_step += 1\n",
    "                        #print(global_step)\n",
    "\n",
    "                        if step % 1000 == 0:\n",
    "                            save_path = saver.save(sess, ckpt_path + \"edsr_ckpt\")\n",
    "                            print(\"Step nr: [{}/{}] - Loss: {:.5f} - Lr: {:.7f}\".format(step, \"?\", \n",
    "                                                                                        float(train_loss/step), l_rate))\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "\n",
    "                save_path = saver.save(sess, ckpt_path + \"edsr_ckpt\")\n",
    "\n",
    "            print(\"Training finished.\")\n",
    "            train_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura EDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5qd1EKZmVt8"
   },
   "outputs": [],
   "source": [
    "class Edsr:\n",
    "\n",
    "    def __init__(self, B, F, scale):\n",
    "        self.B = B\n",
    "        self.F = F\n",
    "        self.scale = scale\n",
    "        self.global_step = tf.compat.v1.placeholder(tf.int32, shape=[], name=\"global_step\")\n",
    "        self.scaling_factor = 0.1\n",
    "        self.bias_initializer = tf.constant_initializer(value=0.0)\n",
    "        self.PS = 3 * (scale*scale) #channels x scale^2\n",
    "        self.xavier = tf.initializers.GlorotUniform()\n",
    "\n",
    "        # -- Filters & Biases --\n",
    "        self.resFilters = list()\n",
    "        self.resBiases = list()\n",
    "\n",
    "        for i in range(0, B*2):\n",
    "            self.resFilters.append(tf.compat.v1.get_variable(\"resFilter%d\" % (i), shape=[3,3,F,F], initializer=self.xavier))\n",
    "            self.resBiases.append(tf.compat.v1.get_variable(name=\"resBias%d\" % (i), shape=[F], initializer=self.bias_initializer))\n",
    "\n",
    "        self.filter_one = tf.compat.v1.get_variable(\"resFilter_one\", shape=[3,3,3,F], initializer=self.xavier)\n",
    "        self.filter_two = tf.compat.v1.get_variable(\"resFilter_two\", shape=[3,3,F,F], initializer=self.xavier)\n",
    "        self.filter_three = tf.compat.v1.get_variable(\"resFilter_three\", shape=[3,3,F,self.PS], initializer=self.xavier)\n",
    "\n",
    "        self.bias_one = tf.compat.v1.get_variable(shape=[F], initializer=self.bias_initializer, name=\"BiasOne\")\n",
    "        self.bias_two = tf.compat.v1.get_variable(shape=[F], initializer=self.bias_initializer, name=\"BiasTwo\")\n",
    "        self.bias_three = tf.compat.v1.get_variable(shape=[self.PS], initializer=self.bias_initializer, name=\"BiasThree\")\n",
    "\n",
    "\n",
    "    def model(self, x, y, lr):\n",
    "        \"\"\"\n",
    "        Implementation of EDSR: https://arxiv.org/abs/1707.02921.\n",
    "        \"\"\"\n",
    "\n",
    "        # -- Model architecture --\n",
    "\n",
    "        # first conv\n",
    "        x = tf.nn.conv2d(x, filters=self.filter_one, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = x + self.bias_one\n",
    "        out1 = tf.identity(x)\n",
    "\n",
    "        # all residual blocks\n",
    "        for i in range(self.B):\n",
    "            x = self.resBlock(x, (i*2))\n",
    "\n",
    "        # last conv\n",
    "        x = tf.nn.conv2d(x, filters=self.filter_two, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = x + self.bias_two\n",
    "        x = x + out1\n",
    "\n",
    "        # upsample via sub-pixel, equivalent to depth to space\n",
    "        x = tf.nn.conv2d(x, filters=self.filter_three, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = x + self.bias_three\n",
    "        out = tf.nn.depth_to_space(x, self.scale, data_format='NHWC', name=\"NHWC_output\")\n",
    "        \n",
    "        # -- --\n",
    "\n",
    "        # some outputs\n",
    "        out_nchw = tf.transpose(out, [0, 3, 1, 2], name=\"NCHW_output\")\n",
    "        psnr = tf.image.psnr(out, y, max_val=255.0)\n",
    "        loss = tf.compat.v1.losses.absolute_difference(out, y) #L1\n",
    "        ssim = tf.image.ssim(out, y, max_val=255.0)\n",
    "        \n",
    "        # (decaying) learning rate\n",
    "        lr = tf.compat.v1.train.exponential_decay(lr,\n",
    "                                        self.global_step,\n",
    "                                        decay_steps=3000,\n",
    "                                        decay_rate=0.95,\n",
    "                                        staircase=True)\n",
    "        # gradient clipping\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(lr)\n",
    "        gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return out, loss, train_op, psnr, ssim, lr\n",
    "\n",
    "    def resBlock(self, inpt, f_nr):\n",
    "        x = tf.nn.conv2d(inpt, filters=self.resFilters[f_nr], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = x + self.resBiases[f_nr]\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = tf.nn.conv2d(x, filters=self.resFilters[f_nr+1], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = x + self.resBiases[f_nr+1]\n",
    "        x = x * self.scaling_factor\n",
    "\n",
    "        return inpt + x"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "EDSRx21.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
