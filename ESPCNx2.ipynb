{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QN7X0D965zC"
   },
   "source": [
    "# Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceucAgEP55op"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "def PSNR(orig, reconstr):\n",
    "    mse = np.mean((orig.astype(float) - reconstr.astype(float)) ** 2)\n",
    "    if mse != 0:\n",
    "        max_pixel = 255.0\n",
    "        return 20 * math.log10(max_pixel / math.sqrt(mse))\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def gen_dataset(filenames, scale):\n",
    "    # The model trains on 17x17 patches\n",
    "    crop_size_lr = 17\n",
    "    crop_size_hr = 17 * scale\n",
    "\n",
    "    for p in filenames:\n",
    "        image_decoded = cv2.imread(p.decode(), 3).astype(np.float32) / 255.0\n",
    "        imgYCC = cv2.cvtColor(image_decoded, cv2.COLOR_BGR2YCrCb)\n",
    "        cropped = imgYCC[0:(imgYCC.shape[0] - (imgYCC.shape[0] % scale)),\n",
    "                  0:(imgYCC.shape[1] - (imgYCC.shape[1] % scale)), :]\n",
    "        lr = cv2.resize(cropped, (int(cropped.shape[1] / scale), int(cropped.shape[0] / scale)),\n",
    "                        interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        hr_y = imgYCC[:, :, 0]\n",
    "        lr_y = lr[:, :, 0]\n",
    "\n",
    "        numx = int(lr.shape[0] / crop_size_lr)\n",
    "        numy = int(lr.shape[1] / crop_size_lr)\n",
    "        for i in range(0, numx):\n",
    "            startx = i * crop_size_lr\n",
    "            endx = (i * crop_size_lr) + crop_size_lr\n",
    "            startx_hr = i * crop_size_hr\n",
    "            endx_hr = (i * crop_size_hr) + crop_size_hr\n",
    "            for j in range(0, numy):\n",
    "                starty = j * crop_size_lr\n",
    "                endy = (j * crop_size_lr) + crop_size_lr\n",
    "                starty_hr = j * crop_size_hr\n",
    "                endy_hr = (j * crop_size_hr) + crop_size_hr\n",
    "\n",
    "                crop_lr = lr_y[startx:endx, starty:endy]\n",
    "                crop_hr = hr_y[startx_hr:endx_hr, starty_hr:endy_hr]\n",
    "\n",
    "                hr = crop_hr.reshape((crop_size_hr, crop_size_hr, 1))\n",
    "                lr = crop_lr.reshape((crop_size_lr, crop_size_lr, 1))\n",
    "                yield lr, hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVlbYwEo7dCE"
   },
   "source": [
    "# Par√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH-jF4mX7SX-"
   },
   "outputs": [],
   "source": [
    "scale = 2\n",
    "epoch = 3\n",
    "testimg = \"/content/drive/MyDrive/TheLastofUs/Test/orig1.png\"\n",
    "lr = 0.0001\n",
    "main_ckpt_dir = \"/content/drive/MyDrive/checkpoints\"\n",
    "if not os.path.exists(main_ckpt_dir):\n",
    "  os.makedirs(main_ckpt_dir)\n",
    "\n",
    "traindir=\"/content/drive/MyDrive/TheLastofUs/HD3\"\n",
    "\n",
    "ARGS = dict()\n",
    "ARGS[\"SCALE\"] = int(scale)\n",
    "ARGS[\"CKPT_dir\"] = main_ckpt_dir + \"/checkpoint\" + \"_sc\" + str(scale)\n",
    "ARGS[\"CKPT\"] = ARGS[\"CKPT_dir\"] + \"/ESPCN_ckpt_sc\" + str(scale)\n",
    "ARGS[\"TRAINDIR\"] = traindir\n",
    "ARGS[\"EPOCH_NUM\"] = int(epoch)\n",
    "ARGS[\"TESTIMG\"] = testimg\n",
    "ARGS[\"LRATE\"] = float(lr)\n",
    "\n",
    "\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_id-Un66oUN"
   },
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0kzOvAH5xMw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "def training(ARGS):\n",
    "    \"\"\"\n",
    "    Start training the ESPCN model.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "\n",
    "    SCALE = ARGS[\"SCALE\"]\n",
    "    BATCH = 16\n",
    "    EPOCHS = ARGS[\"EPOCH_NUM\"]\n",
    "    DATA = pathlib.Path(ARGS[\"TRAINDIR\"])\n",
    "    LRATE = ARGS[\"LRATE\"]\n",
    "\n",
    "    all_image_paths = list(DATA.glob('*'))\n",
    "    all_image_paths = [str(path) for path in all_image_paths]\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        gen_dataset, (tf.float32, tf.float32), (tf.TensorShape([None, None, 1]), tf.TensorShape([None, None, 1])),\n",
    "        args=[all_image_paths, SCALE])\n",
    "    train_dataset = ds.batch(BATCH)\n",
    "    train_dataset = train_dataset.shuffle(10000)\n",
    "    #iter = train_dataset.make_initializable_iterator()\n",
    "    iter = tf.compat.v1.data.make_initializable_iterator(train_dataset)\n",
    "    LR, HR = iter.get_next()\n",
    "    EM = ESPCN(input=LR, scale=SCALE, learning_rate=LRATE)\n",
    "    model = EM.ESPCN_model()\n",
    "    loss, train_op, psnr = EM.ESPCN_trainable_model(model, HR)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_writer = tf.summary.FileWriter('./logs/train', sess.graph)\n",
    "\n",
    "        saver = EM.saver\n",
    "        if not os.path.exists(ARGS[\"CKPT_dir\"]):\n",
    "            os.makedirs(ARGS[\"CKPT_dir\"])\n",
    "        else:\n",
    "            if os.path.isfile(ARGS[\"CKPT\"] + \".meta\"):\n",
    "                saver.restore(sess, tf.train.latest_checkpoint(ARGS[\"CKPT_dir\"]))\n",
    "                print(\"Loaded checkpoint.\")\n",
    "            else:\n",
    "                print(\"Previous checkpoint does not exists.\")\n",
    "\n",
    "        # training with tf.data method\n",
    "        saver = tf.train.Saver()\n",
    "        for e in range(EPOCHS):\n",
    "            sess.run(iter.initializer)\n",
    "            count = 0\n",
    "            train_loss, train_psnr = 0.0, 0.0\n",
    "            while True:\n",
    "                try:\n",
    "                    count = count + 1\n",
    "                    l, t, ps = sess.run([loss, train_op, psnr])\n",
    "                    train_loss += l\n",
    "                    train_psnr += (np.mean(np.asarray(ps)))\n",
    "\n",
    "                    if count % 1000 == 0:\n",
    "                        print(\"Data num:\", '%d' % (count), \"Epoch no:\", '%04d' % (e + 1), \"loss:\", \"{:.9f}\".format(l),\n",
    "                              \"epoch loss:\", \"{:.9f}\".format(train_loss / (count)),\n",
    "                              \"epoch psnr:\", \"{:.9f}\".format(train_psnr / (count)))\n",
    "                        saver.save(sess, ARGS[\"CKPT\"])\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            print(\"END OF EPOCH - Epoch no:\", '%04d' % (e + 1), \"loss:\", \"{:.9f}\".format(float(l)),\n",
    "                  \"epoch loss:\", \"{:.9f}\".format(train_loss / (count)),\n",
    "                  \"epoch psnr:\", \"{:.9f}\".format(train_psnr / (count)))\n",
    "            saver.save(sess, ARGS[\"CKPT\"])\n",
    "\n",
    "        train_writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQbCLVDb7Pv-"
   },
   "source": [
    "# Estructura ESPCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YE_suGyN693m"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "class ESPCN:\n",
    "\n",
    "    def __init__(self, input, scale, learning_rate):\n",
    "        self.LR_input = input\n",
    "        self.scale = scale\n",
    "        self.learning_rate = learning_rate\n",
    "        self.saver = \"\"\n",
    "\n",
    "    def ESPCN_model(self):\n",
    "        \"\"\"\n",
    "        Implementation of ESPCN: https://arxiv.org/abs/1609.05158\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Model\n",
    "        \"\"\"\n",
    "\n",
    "        scale = self.scale\n",
    "        channels = 1\n",
    "        bias_initializer = tf.constant_initializer(value=0.1)\n",
    "        initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "\n",
    "        filters = [\n",
    "            tf.Variable(initializer(shape=(5, 5, channels, 64)), name=\"f1\"),  # (f1,n1) = (5,64)\n",
    "            tf.Variable(initializer(shape=(3, 3, 64, 32)), name=\"f2\"),  # (f2,n2) = (3,32)\n",
    "            tf.Variable(initializer(shape=(3, 3, 32, channels * (scale * scale))), name=\"f3\")  # (f3) = (3)\n",
    "        ]\n",
    "\n",
    "        bias = [\n",
    "            tf.get_variable(shape=[64], initializer=bias_initializer, name=\"b1\"),\n",
    "            tf.get_variable(shape=[32], initializer=bias_initializer, name=\"b2\"),\n",
    "            tf.get_variable(shape=[channels * (scale * scale)], initializer=bias_initializer, name=\"b3\")  # HxWxr^2\n",
    "        ]\n",
    "\n",
    "        l1 = tf.nn.conv2d(self.LR_input, filters[0], [1, 1, 1, 1], padding='SAME', name=\"conv1\")\n",
    "        l1 = l1 + bias[0]\n",
    "        l1 = tf.nn.relu(l1)\n",
    "\n",
    "        l2 = tf.nn.conv2d(l1, filters[1], [1, 1, 1, 1], padding='SAME', name=\"conv2\")\n",
    "        l2 = l2 + bias[1]\n",
    "        l2 = tf.nn.relu(l2)\n",
    "\n",
    "        l3 = tf.nn.conv2d(l2, filters[2], [1, 1, 1, 1], padding='SAME', name=\"conv3\")\n",
    "        l3 = l3 + bias[2]\n",
    "\n",
    "        # Depth_to_space is equivalent to the pixel shuffle layer.\n",
    "        out = tf.nn.depth_to_space(l3, scale, data_format='NHWC')\n",
    "\n",
    "        out = tf.nn.tanh(out, name=\"NHWC_output\")\n",
    "\n",
    "        out_nchw = tf.transpose(out, [0, 3, 1, 2], name=\"NCHW_output\")\n",
    "        # out = tf.nn.relu(out, name=\"NHWC_output\")\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        return out\n",
    "\n",
    "    def ESPCN_trainable_model(self, HR_out, HR_orig):\n",
    "        psnr = tf.image.psnr(HR_out, HR_orig, max_val=1.0)\n",
    "\n",
    "        loss = tf.losses.mean_squared_error(HR_orig, HR_out)\n",
    "\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "\n",
    "        return loss, train_op, psnr\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ESPCNx2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
